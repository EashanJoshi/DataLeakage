Detecting and Recommending Fixes for Multi-Test Data Leakage
This project focuses on identifying and correcting multi-test data leakage in machine learning workflows. Multi-test leakage occurs when test data is unintentionally used during preprocessing, validation, or model selection stages, leading to biased model performance. We developed a lightweight static analysis tool that scans Python scripts, detects common leakage patterns, and recommends practical fixes like K-Fold Cross Validation, proper Train/Validation/Test splits, and pipeline wrapping. The tool outputs annotated scripts and summary reports to help developers build more trustworthy ML models.

To use the tool, simply provide a Python file or folder path, and it will analyze the scripts for leakage issues. Detected problems are flagged directly in the code along with suggested corrections, and an Excel summary is generated for quick review. This project helps catch subtle mistakes early, improving the reproducibility and reliability of machine learning projects.
